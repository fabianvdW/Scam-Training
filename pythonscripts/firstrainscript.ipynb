{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chess\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "DATA_FOLDER = os.path.join(\"..\", \"data\", \"batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = pd.read_csv(os.path.join(DATA_FOLDER, \"batch_28.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 12 * 64 + 1\n",
    "\n",
    "FEATURE_TEMPO =  12  * 64\n",
    "\n",
    "feature_matrix = np.zeros((len(batch), features), dtype=np.int8)\n",
    "labels = np.array(batch.iloc[:, -1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chess import PAWN, KNIGHT, BISHOP, ROOK, QUEEN, KING, WHITE, BLACK, Piece\n",
    "W_PAWN, W_KNIGHT, W_BISHOP, W_ROOK, W_QUEEN, W_KING = Piece(PAWN, WHITE), Piece(KNIGHT, WHITE), Piece(BISHOP, WHITE), Piece(ROOK, WHITE), Piece(QUEEN, WHITE), Piece(KING, WHITE)\n",
    "B_PAWN, B_KNIGHT, B_BISHOP, B_ROOK, B_QUEEN, B_KING = Piece(PAWN, BLACK), Piece(KNIGHT, BLACK), Piece(BISHOP, BLACK), Piece(ROOK, BLACK), Piece(QUEEN, BLACK), Piece(KING, BLACK)\n",
    "semantical_piece_order = [W_PAWN, W_KNIGHT, W_BISHOP, W_ROOK, W_QUEEN, W_KING,\n",
    "                          B_PAWN, B_KNIGHT, B_BISHOP, B_ROOK, B_QUEEN, B_KING]\n",
    "piece_feature_map = { piece: index * 64 for (index, piece) in enumerate(iter(semantical_piece_order))}\n",
    "\n",
    "\n",
    "#Piece Index map is how pieces are defined in Scam, see https://github.com/fabianvdW/Scam/blob/81f8f85bc4f52655b852f87be43546c7dfea6c8c/src/types.rs#L72-L84\n",
    "piece_index_map = {W_PAWN: 1, W_KNIGHT: 2 , W_BISHOP: 3 , W_ROOK: 4 , W_QUEEN: 5 , W_KING: 6 ,\n",
    "                   B_PAWN: 9, B_KNIGHT: 10, B_BISHOP: 11, B_ROOK: 12, B_QUEEN: 13, B_KING: 14}\n",
    "piece_max_index = 15\n",
    "\n",
    "piece_values = {W_PAWN:  100/512, W_KNIGHT:  325/512, W_BISHOP:  350/512, W_ROOK:  550/512, W_QUEEN:  1000/512, W_KING: 0,\n",
    "                B_PAWN: -100/512, B_KNIGHT: -325/512, B_BISHOP: -350/512, B_ROOK: -550/512, B_QUEEN: -1000/512, B_KING: 0}\n",
    "\n",
    "def initialize_piece_values(model):\n",
    "    weights = model.state_dict()['1.weight']\n",
    "    for piece in semantical_piece_order:\n",
    "        for sq in range(64):\n",
    "            weights[0, piece_feature_map[piece] + sq] += piece_values[piece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_features(matrix, at_index, fen):\n",
    "    tokens = fen.split(\" \")\n",
    "    epd, tempo = tokens[0], int({\"w\": 1, \"b\": -1}[tokens[1]])\n",
    "    matrix[at_index, FEATURE_TEMPO] = tempo\n",
    "    board = chess.BaseBoard(board_fen = epd)\n",
    "    for piece in semantical_piece_order:\n",
    "        for sq in board.pieces(piece.piece_type, piece.color):\n",
    "            matrix[at_index, piece_feature_map[piece] + sq] = 1\n",
    "\n",
    "def get_features(fen):\n",
    "    res = np.zeros((1, features), dtype=np.int8)\n",
    "    fill_features(res, 0, fen)\n",
    "    return res\n",
    "def apply_model(model, fen):\n",
    "    return model(torch.tensor(get_features(fen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batch)):\n",
    "    fill_features(feature_matrix, i, batch.iloc[i, 0])\n",
    "np.save(os.path.join(DATA_FOLDER,\"feature_matrix_batch_28\"), feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data, making a train validation split\n",
    "perm = np.random.permutation(len(feature_matrix))\n",
    "feature_matrix = feature_matrix[perm]\n",
    "labels = labels[perm]\n",
    "N = int(0.9 * len(feature_matrix))\n",
    "x_train, x_val, y_train, y_val = feature_matrix[:N], feature_matrix[N:], labels[:N], labels[N:]\n",
    "y_train, y_val = np.expand_dims(y_train, axis = 1), np.expand_dims(y_val, axis = 1)\n",
    "x_train, y_train, x_val, y_val = map(torch.tensor, (x_train, y_train, x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "Train loss: 0.09594909528043535, Val loss: 0.09461022168397903\n",
      "Finished epoch 1\n",
      "Train loss: 0.09459220668501324, Val loss: 0.09428969770669937\n",
      "Finished epoch 2\n",
      "Train loss: 0.09419109808312522, Val loss: 0.09382405132055283\n",
      "Finished epoch 3\n",
      "Train loss: 0.09389035350693596, Val loss: 0.09360276162624359\n",
      "Finished epoch 4\n",
      "Train loss: 0.09361272689183553, Val loss: 0.0932682678103447\n",
      "Finished epoch 5\n",
      "Train loss: 0.0933847198735343, Val loss: 0.09304667264223099\n",
      "Finished epoch 6\n",
      "Train loss: 0.09316931249247656, Val loss: 0.09282287210226059\n",
      "Finished epoch 7\n",
      "Train loss: 0.09296949906720056, Val loss: 0.09267483651638031\n",
      "Finished epoch 8\n",
      "Train loss: 0.09279771267573039, Val loss: 0.09254957735538483\n",
      "Finished epoch 9\n",
      "Train loss: 0.09262937711689208, Val loss: 0.09235011041164398\n",
      "Finished epoch 10\n",
      "Train loss: 0.09246484813637204, Val loss: 0.09223499894142151\n",
      "Finished epoch 11\n",
      "Train loss: 0.09230691453907225, Val loss: 0.09205810725688934\n",
      "Finished epoch 12\n",
      "Train loss: 0.09217182209968566, Val loss: 0.09192004054784775\n",
      "Finished epoch 13\n",
      "Train loss: 0.09203469086196688, Val loss: 0.09186404198408127\n",
      "Finished epoch 14\n",
      "Train loss: 0.09190945776197645, Val loss: 0.09162219613790512\n",
      "Finished epoch 15\n",
      "Train loss: 0.09179287341515223, Val loss: 0.09162163734436035\n",
      "Finished epoch 16\n",
      "Train loss: 0.09167714155276617, Val loss: 0.09143054485321045\n",
      "Finished epoch 17\n",
      "Train loss: 0.09157531572818756, Val loss: 0.09136541932821274\n",
      "Finished epoch 18\n",
      "Train loss: 0.09147525572723812, Val loss: 0.09126914292573929\n",
      "Finished epoch 19\n",
      "Train loss: 0.09137672532399495, Val loss: 0.09128188341856003\n",
      "Finished epoch 20\n",
      "Train loss: 0.09128122048510445, Val loss: 0.0911298468708992\n",
      "Finished epoch 21\n",
      "Train loss: 0.09120728480842379, Val loss: 0.09095136821269989\n",
      "Finished epoch 22\n",
      "Train loss: 0.09111956198003557, Val loss: 0.09092196077108383\n",
      "Finished epoch 23\n",
      "Train loss: 0.09105558139695062, Val loss: 0.09081311523914337\n",
      "Finished epoch 24\n",
      "Train loss: 0.0909788463889228, Val loss: 0.09073176980018616\n",
      "Finished epoch 25\n",
      "Train loss: 0.09090819240967432, Val loss: 0.09066192060709\n",
      "Finished epoch 26\n",
      "Train loss: 0.09084192681471506, Val loss: 0.0906473696231842\n",
      "Finished epoch 27\n",
      "Train loss: 0.09078132885456085, Val loss: 0.09068551659584045\n",
      "Finished epoch 28\n",
      "Train loss: 0.09072728786415524, Val loss: 0.09043656289577484\n",
      "Finished epoch 29\n",
      "Train loss: 0.09066392506784864, Val loss: 0.0906291976571083\n"
     ]
    }
   ],
   "source": [
    "from torch import optim,nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size = 64, shuffle=True)\n",
    "val_ds = TensorDataset(x_val, y_val)\n",
    "val_dl = DataLoader(val_ds, batch_size = 64)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "    \n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def fit(model, epochs, loss_func, opt, train_dl, val_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        for xb, yb in train_dl:\n",
    "            train_loss += loss_batch(model, loss_func, xb, yb, opt)\n",
    "        train_loss /= len(train_ds)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = sum(loss_func(model(xb), yb) for xb, yb in val_dl)\n",
    "            val_loss /= len(val_ds)\n",
    "        print(\"Finished epoch {}\".format(epoch))\n",
    "        print(\"Train loss: {}, Val loss: {}\".format(train_loss, val_loss))\n",
    "        \n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        LambdaLayer(lambda x: x.type(torch.FloatTensor)),\n",
    "        nn.Linear(features, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    loss_func = nn.MSELoss(reduction='sum')\n",
    "    return model, opt, loss_func\n",
    "model, opt, loss_func = get_model()\n",
    "initialize_piece_values(model)\n",
    "fit(model, 30, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), os.path.join(DATA_FOLDER,\"batch_28model.pkl\"))\n",
    "model.load_state_dict(torch.load(os.path.join(DATA_FOLDER, \"batch_28model.pkl\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pub const PSQT: [[i32; 64]; 15] = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ], [26718, 29210, 22080, 25876, 26512, 21141, 26715, 29763, 66162, 69152, 60572, 63500, 63342, 74964, 82719, 61413, 66074, 65381, 58556, 62142, 65883, 68734, 73925, 64307, 70812, 75164, 62970, 63477, 67658, 70119, 76045, 65689, 83038, 84535, 76455, 76180, 78969, 77112, 83845, 67121, 108256, 119883, 121712, 110698, 109552, 108923, 115469, 103648, 173170, 170701, 158304, 145108, 141844, 122494, 125341, 133110, 30067, 26977, 29039, 29656, 22731, 27128, 28616, 28981, ], [115159, 153706, 148022, 158503, 160105, 163225, 148642, 76445, 148303, 168633, 171830, 170477, 172303, 170481, 162843, 162885, 157917, 178394, 176239, 189140, 193126, 185174, 184740, 165133, 165899, 192339, 194880, 197140, 199225, 197454, 199805, 182664, 176509, 195509, 202675, 212549, 208066, 219116, 202866, 201375, 165631, 193066, 211974, 213775, 220947, 221255, 211671, 184225, 162039, 176916, 188518, 204298, 206806, 196218, 177789, 185622, 100840, 180477, 180946, 201042, 185179, 179512, 189499, 118699, ], [188042, 190241, 173434, 181790, 172657, 186015, 181522, 176476, 185378, 192709, 193369, 193657, 188872, 200016, 201542, 198177, 197871, 196853, 201074, 208637, 199888, 203032, 199833, 200410, 202258, 210535, 209767, 211781, 212235, 208245, 202057, 199722, 195432, 204044, 205683, 221440, 216240, 220567, 207636, 202238, 199850, 209650, 221175, 221054, 219817, 228292, 226797, 215681, 183054, 207197, 203975, 213174, 222123, 210097, 211767, 199089, 198898, 202470, 205520, 198496, 206047, 190691, 215454, 188375, ], [294126, 300589, 297922, 305177, 304718, 299453, 308162, 285402, 281855, 287243, 297881, 292113, 294872, 301836, 300986, 295418, 287153, 301046, 296394, 299987, 299826, 294209, 307540, 282573, 298245, 310185, 309754, 311358, 313927, 318211, 316815, 313489, 318099, 326821, 341048, 344190, 330248, 336545, 332252, 326174, 331900, 329586, 337540, 341299, 342307, 351851, 344115, 339980, 339955, 338839, 347867, 343224, 341124, 341393, 340971, 340806, 341001, 344039, 344512, 341724, 343651, 343793, 345801, 346323, ], [530574, 527046, 527651, 534212, 525421, 508102, 510099, 527801, 527630, 540007, 538438, 534389, 537982, 535084, 534297, 530156, 533970, 532594, 545704, 537994, 536611, 542715, 551827, 552614, 525475, 543440, 540221, 546164, 549669, 559137, 561686, 565855, 536927, 546203, 553574, 561344, 579743, 590883, 590775, 574238, 530474, 545999, 550565, 575825, 586050, 605191, 611311, 599718, 546804, 532517, 563311, 573495, 581974, 602192, 578657, 594363, 555900, 576707, 572637, 577630, 583151, 617966, 606519, 601622, ], [-31413, 6768, -11593, -35639, -31298, -24267, -7949, -7040, -182, 1924, -10264, -5870, -7977, -7849, -5456, -8149, -2925, 8433, 2104, 8807, 8929, 8630, -1435, -15801, -4789, 13348, 21586, 17664, 23415, 14962, 10016, -6804, 17742, 30029, 24991, 29691, 28557, 33878, 32046, 6530, 17829, 37170, 33870, 25669, 31651, 35814, 32889, 16748, 21565, 31269, 32219, 8842, 26074, 15696, 38439, -5840, -32076, 6184, 17753, 489, 5046, 28795, -11106, -10218, ], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ], [-22816, -23807, -22100, -25058, -22049, -24951, -25071, -25770, -172166, -173596, -164045, -141227, -145877, -137153, -139489, -139350, -103950, -119832, -120777, -119672, -110491, -101819, -115832, -94126, -81080, -87625, -76035, -76840, -80037, -86124, -82002, -71083, -68650, -76547, -65602, -66985, -71238, -66599, -73717, -65095, -64584, -70629, -58996, -62547, -68899, -67328, -76391, -66012, -63486, -67740, -63382, -64022, -58071, -77174, -83702, -64938, -29704, -21149, -29596, -22033, -25089, -26729, -23957, -28918, ], [-102258, -184303, -176917, -186353, -201939, -198232, -200389, -102983, -165372, -178987, -197286, -208150, -213366, -201860, -184224, -175828, -165976, -194387, -205759, -210059, -228530, -231472, -204566, -197858, -173408, -187206, -201401, -210111, -210699, -220761, -195322, -196212, -178372, -194375, -193697, -196338, -204266, -197931, -198987, -172254, -159609, -169797, -174115, -201889, -194920, -185555, -174832, -167708, -150712, -154933, -171383, -178608, -178906, -182022, -162734, -156897, -114777, -155315, -142241, -157194, -157503, -159141, -155786, -83461, ], [-185122, -193534, -211851, -194135, -208234, -189251, -215735, -210555, -183598, -213028, -206757, -214584, -211960, -211753, -201607, -209680, -200587, -210163, -216164, -214413, -218644, -218036, -224260, -213876, -193317, -214434, -215716, -226205, -223958, -217930, -210598, -216849, -195352, -209132, -212751, -215525, -214786, -211519, -215524, -210488, -189628, -202144, -206124, -208593, -199728, -198604, -202237, -201232, -189303, -192968, -195937, -190211, -195668, -194027, -198231, -200787, -191082, -189275, -178962, -183530, -182186, -184441, -189255, -195178, ], [-334515, -340169, -345413, -343359, -336442, -339925, -342612, -346015, -332345, -334533, -346479, -344426, -351464, -342523, -336567, -337478, -333827, -332792, -339629, -340485, -340268, -346477, -338358, -340770, -321677, -324081, -326122, -327319, -335242, -336279, -329348, -329250, -306174, -313239, -313667, -316821, -305677, -307230, -313150, -304596, -293073, -301567, -302745, -301580, -311181, -289974, -303427, -300628, -270942, -280634, -294564, -293775, -289918, -292405, -300477, -285725, -293781, -297410, -304836, -306792, -302905, -300367, -303731, -285773, ], [-557890, -573392, -575766, -584098, -594380, -590987, -594705, -602041, -547409, -539827, -560746, -588817, -593410, -601850, -563214, -596353, -542116, -542454, -554208, -563094, -573301, -608679, -609016, -575896, -538391, -541787, -545212, -557880, -581857, -572598, -579985, -568476, -531778, -545531, -547721, -559219, -554184, -559716, -553366, -563907, -525722, -533804, -553214, -539980, -534911, -536965, -548966, -542853, -531116, -539576, -535635, -541361, -537289, -538418, -525550, -533219, -538602, -533162, -524824, -533126, -528096, -513131, -519056, -509198, ], [47304, -22762, -2357, -18814, 26077, 17663, 6209, 50968, -16426, -33250, -10505, 7727, -2135, -15554, -31904, 3714, -34218, -40410, -27807, -23944, -11681, -32281, -43194, -23291, -16693, -31164, -36725, -17600, -30234, -24783, -17152, -6557, 10252, -18503, -23948, -17153, -18450, -16119, -18099, 7566, 11367, -960, -3920, -8094, -8730, -4548, 1109, 14286, 4017, 259, 7988, 13186, 13066, 3814, 4062, 5151, 22807, -9438, 3009, 27002, 33119, 24701, 9599, 14631, ], ];\n",
      "pub const TEMPO_BONUS: i32 = 3775;\n",
      "pub const BIAS: i32 = 1120;\n",
      "pub const DIV: i32 = 512;\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "weights = params[0].detach().numpy()[0]\n",
    "bias = params[1].detach().numpy()[0]\n",
    "sc = 2**17\n",
    "shift = 2**9\n",
    "def scale(w):\n",
    "    return round(sc * w)\n",
    "def print_psqt(weights):\n",
    "    res = np.zeros((15, 64))\n",
    "    for piece in semantical_piece_order:\n",
    "        for sq in range(64):\n",
    "            res[piece_index_map[piece], sq] = scale(weights[piece_feature_map[piece] + sq])\n",
    "    res_str = \"[\"\n",
    "    for i in range(len(res)):\n",
    "        res_str += \"[\"\n",
    "        for j in range(len(res[i])):\n",
    "            res_str += str(int(res[i, j])) +\", \"\n",
    "        res_str += \"], \"\n",
    "    res_str += \"]\"\n",
    "    return res_str\n",
    "rust_psqt = \"pub const PSQT: [[i32; 64]; {}] = {};\".format(piece_max_index, print_psqt(weights[:-1]))\n",
    "rust_tempo_bonus = \"pub const TEMPO_BONUS: i32 = {};\".format(scale(weights[-1]))\n",
    "rust_bias = \"pub const BIAS: i32 = {};\".format(scale(bias))\n",
    "rust_shift = \"pub const DIV: i32 = {};\".format(shift)\n",
    "print(rust_psqt)\n",
    "print(rust_tempo_bonus)\n",
    "print(rust_bias)\n",
    "print(rust_shift)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
