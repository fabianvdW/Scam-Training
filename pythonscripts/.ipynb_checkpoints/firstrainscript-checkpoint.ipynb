{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chess\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "DATA_FOLDER = os.path.join(\"..\", \"data\", \"batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = pd.read_csv(os.path.join(DATA_FOLDER, \"batch_23.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 12 * 64 + 1\n",
    "\n",
    "FEATURE_TEMPO =  12  * 64\n",
    "\n",
    "feature_matrix = np.zeros((len(batch), features), dtype=np.uint8)\n",
    "feature_matrix = np.load(os.path.join(DATA_FOLDER,\"feature_matrix_batch_23.npy\"))\n",
    "labels = np.array(batch.iloc[:, -1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chess import PAWN, KNIGHT, BISHOP, ROOK, QUEEN, KING, WHITE, BLACK, Piece\n",
    "W_PAWN, W_KNIGHT, W_BISHOP, W_ROOK, W_QUEEN, W_KING = Piece(PAWN, WHITE), Piece(KNIGHT, WHITE), Piece(BISHOP, WHITE), Piece(ROOK, WHITE), Piece(QUEEN, WHITE), Piece(KING, WHITE)\n",
    "B_PAWN, B_KNIGHT, B_BISHOP, B_ROOK, B_QUEEN, B_KING = Piece(PAWN, BLACK), Piece(KNIGHT, BLACK), Piece(BISHOP, BLACK), Piece(ROOK, BLACK), Piece(QUEEN, BLACK), Piece(KING, BLACK)\n",
    "semantical_piece_order = [W_PAWN, W_KNIGHT, W_BISHOP, W_ROOK, W_QUEEN, W_KING,\n",
    "                          B_PAWN, B_KNIGHT, B_BISHOP, B_ROOK, B_QUEEN, B_KING]\n",
    "piece_feature_map = { piece: index * 64 for (index, piece) in enumerate(iter(semantical_piece_order))}\n",
    "\n",
    "\n",
    "#Piece Index map is how pieces are defined in Scam, see https://github.com/fabianvdW/Scam/blob/81f8f85bc4f52655b852f87be43546c7dfea6c8c/src/types.rs#L72-L84\n",
    "piece_index_map = {W_PAWN: 1, W_KNIGHT: 2 , W_BISHOP: 3 , W_ROOK: 4 , W_QUEEN: 5 , W_KING: 6 ,\n",
    "                   B_PAWN: 9, B_KNIGHT: 10, B_BISHOP: 11, B_ROOK: 12, B_QUEEN: 13, B_KING: 14}\n",
    "piece_max_index = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batch)):\n",
    "    tokens = batch.iloc[i, 0].split(\" \")\n",
    "    epd, tempo = tokens[0], int({\"w\": WHITE, \"b\":BLACK}[tokens[1]])\n",
    "    feature_matrix[i, FEATURE_TEMPO] = tempo\n",
    "    board = chess.BaseBoard(board_fen = epd)\n",
    "    for piece in iter(semantical_piece_order):\n",
    "        for sq in board.pieces(piece.piece_type, piece.color):\n",
    "            feature_matrix[i, piece_feature_map[piece] + sq] = 1\n",
    "np.save(os.path.join(DATA_FOLDER,\"feature_matrix_batch_23\"), feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data, making a train validation split\n",
    "perm = np.random.permutation(len(feature_matrix))\n",
    "feature_matrix = feature_matrix[perm]\n",
    "labels = labels[perm]\n",
    "N = int(0.9 * len(feature_matrix))\n",
    "x_train, x_val, y_train, y_val = feature_matrix[:N], feature_matrix[N:], labels[:N], labels[N:]\n",
    "y_train, y_val = np.expand_dims(y_train, axis = 1), np.expand_dims(y_val, axis = 1)\n",
    "x_train, y_train, x_val, y_val = map(torch.tensor, (x_train, y_train, x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "Train loss: 0.12864980922275118, Val loss: 0.12646682560443878\n",
      "Finished epoch 1\n",
      "Train loss: 0.12560287529203626, Val loss: 0.12569324672222137\n",
      "Finished epoch 2\n",
      "Train loss: 0.1250071109173033, Val loss: 0.12523220479488373\n",
      "Finished epoch 3\n",
      "Train loss: 0.12457060260825686, Val loss: 0.12474074214696884\n",
      "Finished epoch 4\n",
      "Train loss: 0.12426478841967054, Val loss: 0.12463343888521194\n",
      "Finished epoch 5\n",
      "Train loss: 0.12397681601683298, Val loss: 0.124379001557827\n",
      "Finished epoch 6\n",
      "Train loss: 0.12373610164191988, Val loss: 0.12401709705591202\n",
      "Finished epoch 7\n",
      "Train loss: 0.12353843417432574, Val loss: 0.1238693967461586\n",
      "Finished epoch 8\n",
      "Train loss: 0.12335757310814327, Val loss: 0.12383292615413666\n",
      "Finished epoch 9\n",
      "Train loss: 0.12320515643543667, Val loss: 0.12359990924596786\n"
     ]
    }
   ],
   "source": [
    "from torch import optim,nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size = 64, shuffle=True)\n",
    "val_ds = TensorDataset(x_val, y_val)\n",
    "val_dl = DataLoader(val_ds, batch_size = 64)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "    \n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def fit(model, epochs, loss_func, opt, train_dl, val_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        for xb, yb in train_dl:\n",
    "            train_loss += loss_batch(model, loss_func, xb, yb, opt)\n",
    "        train_loss /= len(train_ds)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = sum(loss_func(model(xb), yb) for xb, yb in val_dl)\n",
    "            val_loss /= len(val_ds)\n",
    "        print(\"Finished epoch {}\".format(epoch))\n",
    "        print(\"Train loss: {}, Val loss: {}\".format(train_loss, val_loss))\n",
    "        \n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        LambdaLayer(lambda x: x.type(torch.FloatTensor)),\n",
    "        nn.Linear(features, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    loss_func = nn.MSELoss(reduction='sum')\n",
    "    return model, opt, loss_func\n",
    "model, opt, loss_func = get_model()\n",
    "fit(model, 10, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(DATA_FOLDER,\"batch_23model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ...\n",
    "bias = ...\n",
    "def print_psqt(weights):\n",
    "    pass\n",
    "rust_psqt = \"pub const PSQT: [[f32; 64]; {}] = {};\".format(piece_max_index, print_psqt(weights[:--1]))\n",
    "rust_tempo_bonus = \"pub const TEMPO_BONUS: f32 = {};\".format(weights[-1])\n",
    "rust_bias = \"pub const BIAS: f32 = {};\".format(bias)\n",
    "print(rust_psqt)\n",
    "print(rust_tempo_bonus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
